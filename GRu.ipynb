{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02766ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c77aef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du device : cpu\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Data/clean.csv\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Utilisation du device : {device}\")\n",
    "\n",
    "# Normalisation\n",
    "dataset = data[['Close']].values\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_scaled = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e08d4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des séquences\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:(i + seq_length)])\n",
    "        ys.append(data[i + seq_length])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "X, y = create_sequences(data_scaled, SEQ_LENGTH)\n",
    "\n",
    "# Conversion PyTorch\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Split 80/10/10\n",
    "train_size = int(len(X) * 0.8)\n",
    "val_size = int(len(X) * 0.1)\n",
    "\n",
    "X_train, y_train = X_tensor[:train_size], y_tensor[:train_size]\n",
    "X_val, y_val = X_tensor[train_size:train_size+val_size], y_tensor[train_size:train_size+val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e25e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), shuffle=False, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84ff2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Notez l'utilisation de nn.GRU au lieu de nn.LSTM\n",
    "        self.gru = nn.GRU(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Couche fully connected\n",
    "        # Pas de *2 ici car ce n'est pas un Bi-GRU (sauf si on ajoute bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialisation du hidden state uniquement (pas de c0 pour le GRU)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # On récupère la dernière étape de temps\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb2e1ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-07 13:03:12,575]\u001b[0m A new study created in memory with name: no-name-59abae33-9b3f-41d0-9004-65b9b0261a41\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche des meilleurs hyperparamètres pour le GRU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-07 13:04:25,083]\u001b[0m Trial 0 finished with value: 0.0016900061891647056 and parameters: {'hidden_size': 187, 'num_layers': 2, 'dropout': 0.31184887193254296, 'lr': 0.0007275799514514403}. Best is trial 0 with value: 0.0016900061891647056.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:05:01,868]\u001b[0m Trial 1 finished with value: 0.014073827560059727 and parameters: {'hidden_size': 175, 'num_layers': 1, 'dropout': 0.48481988554520716, 'lr': 0.008471617370545598}. Best is trial 0 with value: 0.0016900061891647056.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:06:02,817]\u001b[0m Trial 2 finished with value: 0.053048030869103965 and parameters: {'hidden_size': 216, 'num_layers': 2, 'dropout': 0.2779406400523725, 'lr': 0.008269037020145118}. Best is trial 0 with value: 0.0016900061891647056.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:07:10,789]\u001b[0m Trial 3 finished with value: 0.003948730739648454 and parameters: {'hidden_size': 227, 'num_layers': 2, 'dropout': 0.4748680117347296, 'lr': 0.00072322741590765}. Best is trial 0 with value: 0.0016900061891647056.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:07:48,714]\u001b[0m Trial 4 finished with value: 0.0027597043779678644 and parameters: {'hidden_size': 119, 'num_layers': 2, 'dropout': 0.35861797967346165, 'lr': 0.00013772806403498176}. Best is trial 0 with value: 0.0016900061891647056.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:07:56,023]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:08:08,434]\u001b[0m Trial 6 finished with value: 0.001955072178679984 and parameters: {'hidden_size': 77, 'num_layers': 1, 'dropout': 0.29059008230031763, 'lr': 0.003834392184436689}. Best is trial 0 with value: 0.0016900061891647056.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:08:29,643]\u001b[0m Trial 7 finished with value: 0.002216792680701474 and parameters: {'hidden_size': 174, 'num_layers': 1, 'dropout': 0.1574583403466192, 'lr': 0.0026033974844313084}. Best is trial 0 with value: 0.0016900061891647056.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:08:35,120]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:08:40,107]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:08:42,309]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:08:51,936]\u001b[0m Trial 11 finished with value: 0.0017687281040707603 and parameters: {'hidden_size': 48, 'num_layers': 1, 'dropout': 0.23791868528852977, 'lr': 0.002191887035785804}. Best is trial 0 with value: 0.0016900061891647056.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:08:52,896]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:09:14,326]\u001b[0m Trial 13 finished with value: 0.0018754093849565833 and parameters: {'hidden_size': 192, 'num_layers': 1, 'dropout': 0.37920370670649245, 'lr': 0.0013809519103273112}. Best is trial 0 with value: 0.0016900061891647056.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:10:22,452]\u001b[0m Trial 14 finished with value: 0.0010666836824384518 and parameters: {'hidden_size': 242, 'num_layers': 2, 'dropout': 0.219326139247767, 'lr': 0.00041706050465717024}. Best is trial 14 with value: 0.0010666836824384518.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:11:39,120]\u001b[0m Trial 15 finished with value: 0.00136787540250225 and parameters: {'hidden_size': 251, 'num_layers': 2, 'dropout': 0.3996428568278656, 'lr': 0.0003594576896768347}. Best is trial 14 with value: 0.0010666836824384518.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:12:59,027]\u001b[0m Trial 16 finished with value: 0.0012012745486572386 and parameters: {'hidden_size': 255, 'num_layers': 2, 'dropout': 0.4296924059889228, 'lr': 0.00036167464330027447}. Best is trial 14 with value: 0.0010666836824384518.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:13:10,610]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:14:16,523]\u001b[0m Trial 18 finished with value: 0.0012466323416447267 and parameters: {'hidden_size': 226, 'num_layers': 2, 'dropout': 0.41760818418704776, 'lr': 0.0001168785890172322}. Best is trial 14 with value: 0.0010666836824384518.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:15:17,819]\u001b[0m Trial 19 finished with value: 0.0010635943282977678 and parameters: {'hidden_size': 211, 'num_layers': 2, 'dropout': 0.23754761354172282, 'lr': 0.0004632766293278801}. Best is trial 19 with value: 0.0010635943282977678.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:15:27,347]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:15:34,287]\u001b[0m Trial 21 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:16:41,734]\u001b[0m Trial 22 finished with value: 0.0012555649474961684 and parameters: {'hidden_size': 233, 'num_layers': 2, 'dropout': 0.25679848681767364, 'lr': 0.00022417272271530413}. Best is trial 19 with value: 0.0010635943282977678.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:16:47,242]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:16:51,828]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:18:02,530]\u001b[0m Trial 25 finished with value: 0.001228879819973372 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.43675804554522274, 'lr': 0.00024370512013260407}. Best is trial 19 with value: 0.0010635943282977678.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:18:11,998]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:18:18,943]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:19:21,725]\u001b[0m Trial 28 finished with value: 0.0012208019354147837 and parameters: {'hidden_size': 218, 'num_layers': 2, 'dropout': 0.2222205578646814, 'lr': 0.0001529448489389161}. Best is trial 19 with value: 0.0010635943282977678.\u001b[0m\n",
      "\u001b[32m[I 2026-02-07 13:19:27,148]\u001b[0m Trial 29 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres trouvés : {'hidden_size': 211, 'num_layers': 2, 'dropout': 0.23754761354172282, 'lr': 0.0004632766293278801}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # --- Espace de recherche ---\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 256)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    # --- Création du modèle ---\n",
    "    model = GRUModel(input_size=1, hidden_size=hidden_size, num_layers=num_layers, output_size=1, dropout=dropout).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # Adam est souvent meilleur pour GRU aussi\n",
    "    \n",
    "    # --- Entraînement rapide (Pruning) ---\n",
    "    for epoch in range(10): # 10 époques suffisent pour voir si l'architecture est bonne\n",
    "        model.train()\n",
    "        for x_b, y_b in train_loader:\n",
    "            x_b, y_b = x_b.to(device), y_b.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_b)\n",
    "            loss = criterion(out, y_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x_v, y_v in val_loader:\n",
    "                x_v, y_v = x_v.to(device), y_v.to(device)\n",
    "                pred = model(x_v)\n",
    "                val_losses.append(criterion(pred, y_v).item())\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        # Signaler à Optuna pour qu'il arrête si c'est mauvais (Pruning)\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return avg_val_loss\n",
    "\n",
    "print(\"Recherche des meilleurs hyperparamètres pour le GRU...\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30) # 30 essais\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"Meilleurs paramètres trouvés : {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dee87ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du modèle final avec les meilleurs paramètres...\n",
      "Epoch 1/100 | Train Loss: 0.078972 | Val Loss: 0.027329\n",
      "  -> Modèle GRU sauvegardé !\n",
      "Epoch 2/100 | Train Loss: 0.031797 | Val Loss: 0.021063\n",
      "  -> Modèle GRU sauvegardé !\n",
      "Epoch 3/100 | Train Loss: 0.024040 | Val Loss: 0.016547\n",
      "  -> Modèle GRU sauvegardé !\n",
      "Epoch 4/100 | Train Loss: 0.018024 | Val Loss: 0.012772\n",
      "  -> Modèle GRU sauvegardé !\n",
      "Epoch 5/100 | Train Loss: 0.013091 | Val Loss: 0.009112\n",
      "  -> Modèle GRU sauvegardé !\n",
      "Epoch 6/100 | Train Loss: 0.008714 | Val Loss: 0.005966\n",
      "  -> Modèle GRU sauvegardé !\n",
      "Epoch 7/100 | Train Loss: 0.004986 | Val Loss: 0.003252\n",
      "  -> Modèle GRU sauvegardé !\n",
      "Epoch 8/100 | Train Loss: 0.002540 | Val Loss: 0.001777\n",
      "  -> Modèle GRU sauvegardé !\n",
      "Epoch 9/100 | Train Loss: 0.001228 | Val Loss: 0.001258\n",
      "  -> Modèle GRU sauvegardé !\n",
      "Epoch 10/100 | Train Loss: 0.000703 | Val Loss: 0.001181\n",
      "  -> Modèle GRU sauvegardé !\n",
      "Epoch 11/100 | Train Loss: 0.000644 | Val Loss: 0.001191\n",
      "Epoch 12/100 | Train Loss: 0.000668 | Val Loss: 0.001319\n",
      "Epoch 13/100 | Train Loss: 0.000727 | Val Loss: 0.001412\n",
      "Epoch 14/100 | Train Loss: 0.000778 | Val Loss: 0.001423\n",
      "Epoch 15/100 | Train Loss: 0.000848 | Val Loss: 0.001603\n",
      "Epoch 16/100 | Train Loss: 0.001022 | Val Loss: 0.001748\n",
      "Epoch 17/100 | Train Loss: 0.001231 | Val Loss: 0.002005\n",
      "Epoch 18/100 | Train Loss: 0.001430 | Val Loss: 0.002031\n",
      "Epoch 19/100 | Train Loss: 0.001613 | Val Loss: 0.001976\n",
      "Epoch 20/100 | Train Loss: 0.001739 | Val Loss: 0.001907\n",
      "Epoch 21/100 | Train Loss: 0.001931 | Val Loss: 0.002120\n",
      "Epoch 22/100 | Train Loss: 0.002418 | Val Loss: 0.002472\n",
      "Epoch 23/100 | Train Loss: 0.003084 | Val Loss: 0.003302\n",
      "Epoch 24/100 | Train Loss: 0.004290 | Val Loss: 0.005408\n",
      "Epoch 25/100 | Train Loss: 0.006525 | Val Loss: 0.007718\n",
      "Arrêt précoce à l'époque 25\n",
      "Terminé. Le modèle est sauvegardé sous 'best_gru_nflx.pth'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEntraînement du modèle final avec les meilleurs paramètres...\")\n",
    "\n",
    "final_model = GRUModel(\n",
    "    input_size=1,\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    output_size=1,\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Paramètres d'entraînement complet\n",
    "NUM_EPOCHS = 100\n",
    "patience = 15\n",
    "best_val_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    final_model.train()\n",
    "    batch_losses = []\n",
    "    for x_b, y_b in train_loader:\n",
    "        x_b, y_b = x_b.to(device), y_b.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = final_model(x_b)\n",
    "        loss = criterion(out, y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    train_loss = np.mean(batch_losses)\n",
    "    \n",
    "    final_model.eval()\n",
    "    val_batch_losses = []\n",
    "    with torch.no_grad():\n",
    "        for x_v, y_v in val_loader:\n",
    "            x_v, y_v = x_v.to(device), y_v.to(device)\n",
    "            pred = final_model(x_v)\n",
    "            val_batch_losses.append(criterion(pred, y_v).item())\n",
    "            \n",
    "    val_loss = np.mean(val_batch_losses)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    # Early Stopping & Sauvegarde\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(final_model.state_dict(), 'best_gru_nflx.pth') # Sauvegarde sous un nom différent\n",
    "        print(\"  -> Modèle GRU sauvegardé !\")\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(f\"Arrêt précoce à l'époque {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"Terminé. Le modèle est sauvegardé sous 'best_gru_nflx.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
